Sen:

- providing the skills to make it easier to use agent for the user (ecosystem of skills)
- what are realistic inputs for end-to-end results? 
- interactive - but allowing for long horizon tasks and is also immediately useful - enough cool skills and add different tools
- maybe its load this model and steering vector - maybe replciate this paper have some hand holding and interactive agent
- let claude code do better subtasks on the agent
- a few videos of doing a variety of investigations - opening claude code and replicate the introspection paper - starting claude code and have it run. Use claude code to do your reasearch
- using claude code to do research directly - have some variety of tasks and makshould we e videos. fairly mindblowing to have it do steering vectors
- have an autorater that evals a steering result
- have a checkpoint defined for when u get feedback?
- whats better as a ui for cli vs notebook?
- woult it be useful or release it via claude 

Two different use cases:
1. use it to conduct experiments on agents and do measurements
    a.) release chinese model investigations on this infra?
2. use it as a researcher tool
    b.) release modal infra mcp server for researchers to use with claude code


----------------------------------


What has been done since & current state
tldr; prioritizing making it modular, hackable and a solid base to then move towards making it a research tool or just deploying it standalone
1. Infrastructure overhaul to use modal sandboxes and loading arbitrary environments
2. Writing agent harnesses that runs Claude code non interactively to test different harnesses - petri can be called here as a subagent 
3. An interactive mode to use our env infra as an Mcp server and run experiments as a researcher
Spending a GOD AWFUL amoutn of time on docker in docker for deploying - going to be better about this and prioritize away

What is the most impactful NXT step? What is a good thing to aim for to release by Dec 15th. Roughly 2 week timeline

Who will it be useful for?
- people who actively want to try building better agents and testing agent capabilities fr eg. People wanting to hill climb to make better investigator agents?
- People who want to try doing research with agents in general - researcher tool

Agent infra (this can also be a researcher tool - cc Julian):
- Can release infra as is with docs and some experiments on Chinese models we did we investigator agents built on the infra
    - Run petri on the same infra to benchmark against these investigator agents
    - Create a set of skills that the agent uses that people using this infra can also leverage directly
- Test multiple different agent harnesses and how they perform on the given task and measure stuff like
    - what techniques or skills are most useful?
    - how should these techniques be formatted or supplied to do best?

Researcher tool: (evidence ish is Sen is interested lol and Clement and Cam (& Johnny lin) too - but real test is whether they actually would use it)
- Let people use our infra plugin directly with Claude code via an mcp
    - modal-infra mcp helps solve the issue of env setup and getting the env to a place where agents can actually use it meaningfully. it builds the env, loads containers, downloads models, prepares skills etc
- Make the best open source investigator agent with the combo of the above infra and adding it in our harness - make this runabble in claude code as a subagent plugin as well
- Create a more explicit website that allows selecting tools and running the agent on a task
- Create a set of skills for claude to use that allows it to use whitebox stuff (just .md files)
- ship on neuronpedia as api like /agent/task (have ui to equip skills exactly like this and hit run)


- if its actually useful the researcher can suck it up lol
- make 1 happen well aka agent infra- 
- maybe too mcuh drudge work for teh researcher to use it
- think abut how long I wanna maintain it - 5 - 10 hours
- record several videos doing basic things with the library, docs and tutorials  
- "u can run petri in this" 
1. show what it can do
2. show how easily hackable it is
- how easily can i use this with cursor ie can ai agents use things


papers:
try refusal is mediated by a single direction: pick x model and replicate the experiment in figure 1 on this model.
activation addition paper from alex turner
goodfire weights paper
do I know this entity paper - used existing SAEs to find SAE latents

why would someone care/use this?
- whats important is enabling research projects on agents - having an agent do x in a research projects
- hackable petri
- can we create an agent that does automated red teaming ie breaks through an autorater (maybe break through petri)
- create prompts to make a model deceptive - make a model red teaming make an okay version of this
- environment creator and as an environment
- make a patient that realistically exhibits psychosis in the model make an env for this.


two things that matter (niche audience of researchers)
- see a thing that they care about being doable
- hackable



1. make this robust to people not knowing the paper they are replicating
2. make a config making agent maybe that just works? try opus 4.5


- replicating a paper vs replicating an environments
- seeing how hard it is to make an agent for a given task - use bartosz user female model.
- 


- questions for sen: what would need to be in the mvp for him to use it
- send a message for call


=========wil

notes from videos

1. fix the refresh thing bug and make it easy to see where the agent is working currently
2. test with actually telling claude what you would want and intervening on the subagent flow
3. have a code quality and style guidance prompt that we append
4. stop after experiment is done




===========

what might be needed for release?

- can i come in like julian etc. and just put in a technique that i made relatively easily and run it against a bunch of tasks?
- can we have a hackable playable version of petri as a harness
- can we spawn an environment with an autorater where another agent does automated red teaming? ie tries to break through an autorater that is flagging for malicious prompts?
- can we make an agent that can control the prompts given to another agent that is talking to a target model and see which prompt most reliably gets the target to exhibit psychosis inducing behaviour


seperately code wise
- i need to have an environments abstraction that has providers with clients for each modal etc.
- i need to have execution support for just cli, or mcp tool calling
- i need to have better harnesses and slightly more diversity


- check how having an api with a hidden prompt might work - is this the bes tthiing to do?
- maybe for things like when we have an autorater we wrap up the env with api in a docker container that the agent interacts with in some more standardized way



===== wip abstractions ====

environment is the setup stage. it prepares the file system installs anyhting, github repos, docker containers, downloads models etc.
needs:
- image (the container containing the thing to run)
- interface (indicates what needs to be made available to the agent in its exeuction)

at the end it returns out the interface ie things the agent needs access to which the execution layer uses to decide
how to expose it to it

execution (takes in an interface of stuff to expose -> runs and exposes it to agent -> session id):
notebook:
- loads any models
- runs docker container and adds functions to access it into namespace
- loads the skills dictionary and available skills into namespace?
cli:
- creates a script that loads in models
- runs script to run docker container
- has a skills.md folder for agent to use
mcp:
god knows


Clean flow:
environment: f(image, interface) -> clones repo, downloads model, prepares file system -> interface, sandbox_url
execution: f(interface, sandbox_url) -> loads in interface, creates model objects, runs container and exposes functions -> session id
harness: connects agents to a given session id and fills in prompts.

implementation details:
we seemingly need a docker container runner for this that takes a repo, builds and runs it, exposes the api
our environment builder would use this service to run containers ie github repos people define



really have 3 seperate investigations
- white box model
- shutdown ie modify repo run some experment and play with environment and tool calls
- given a scoped environment with xyz available functions come up with best x that does y


level of scope is the main factor here
environment might have another property of "scoped" if it is scoped then the specific repos or whatever will scope 
whats available and run in a container that the agent cant access

=====
Sandbox, ScopedSandbox

ScopedSandbox exposes an interface

This interface can be added as mcp tools directly to the agent harness, it can be exposed in the name space of the notebook in the agents notebook session, it can be exposed as a set of skills in the agents
working directory when its using the cli effectively installed as a plugin. 

The question here really is about plugins and how the plugin is served. it can be an mcp tool, it could also be in the form of a specialized subagent, it could be a set of discoverable
skills that are added to the cli session, it can be exposed as a namespace in the notebook it can be mcp tools directly added to teh agent harness it can be just prompts in the agent

Another insight is that the session or execution mode is ALSO just a plugin in the form of how the agent connects to a sandbox, in notebook sessions it just gets an mcp server to execute on a notebook
in a cli session same thing. alternatively it could also run in like "local" and gets a workspace folder locally and has mcp tool calls it can call that are custom and exposed
via a scoped sandbox interface.

We can also compose these two where tha gent has access to a sandbox with a notebook session and that sandbox's session? has injected into its namespace anotehr scoped sandbox's interface
etc.

----
sandbox = Sandbox.create()

//create a session (this is mcp tools for the agent!)
session = Session("cli")
session.connect(sandbox)

agent(session) #this adds mcp tools to the agent to use
---

interface = Interface("./targets/petri_target.py")
target = ScopedSandbox.create()
target.serve_interface(interface)
target.start()

mcp_config = interface.as_mcp_config()

session = Session(mcp_config)
agent(session)

here the agent connects to a session with only that base mcp config
--- 
sandbox = Sandbox.create()

//create a session (this is mcp tools for the agent!)
session = Session("cli")
session.connect(sandbox)

interface = Interface("./targets/petri_target.py")
target = ScopedSandbox.create()
target.serve_interface(interface)
target.start()

interface_as_notebook_namespace_injection = interface.to_notebook_injections()
session.inject(interface_as_notebook_namespace_injection)

agent(session) #now the agent has mcp tools for the cli since that is the session, and has a notebook session with injections to call the interface
-----



  from interp_infra.harness import Task, Harness
  from interp_infra.interfaces import InterfaceSpec
  from interp_infra.scoped import ScopedSandbox

  env = Environment()

  config = SandboxConfig(
      base_image="ghcr.io/your-org/interp-base:latest",
      packages=["torch", "transformers"],
      execution_mode=ExecutionMode.NOTEBOOK,  # or CLI / LOCAL
  )

  sandbox = env.create_sandbox(config)
  session = sandbox.create_session()  # gives a NotebookSession / CLISession / LocalSession
  agent(session)

 --


from interp_infra.harness import Task, Harness
  from interp_infra.interfaces import InterfaceSpec
  from interp_infra.scoped import ScopedSandbox

  env = Environment()

  config = SandboxConfig(
      execution_mode=ExecutionMode.NOTEBOOK,  # or CLI / LOCAL
  )

  sandbox = env.create_sandbox(config)
  session = sandbox.create_session()  # gives a NotebookSession / CLISession / LocalSession
  
  //but we also want to supply this session with information on another sandbox and available methods

  saScopedSandbox(repo: "some repo", run: "how to run", whatever other setup, interface=what to expose)


session has a .add function that allows for adding interfaces into it.
interfaces are a set of code + prompts and has both those components.
- primarily an interface can be converted into an mcp server that is then hosted as an mcp server and is available as toool calls for the agent harness directly
- an interface can be converted into a set of files of .md in a folder for the agent to discover in its workspace 
- it can be loaded into the namespace of the notebook if running in notebook mode
- can be ONLY supplied into the prompt of the harness/agent so not in the workspace in any way
- they can be objects and 

- skill files discoverable by claude
- injected into the namespace of notebook if in notebook mode
- as code files that can be directly callable and has a prompt com

another annoying thing is that interfaces that are defined on a scoped sandbox will usually reference code etc. and are pretty fundamentally different from like
a technique or specialized prompt + isolated set of code we want the agent to call.

maybe worth seperating these. however how they are handled and added into the agent/session is pretty much the same we can do anything wth them. 
ig the only differnece is that the code on these is a proxy server that is then handled on it directly.

So theres the interface you define for a scoped sandbox
Theres the set of code + prompts that might constitute an interp technique, or how to use a different library

And theres How these are integrated into the agent/session 


  Petri's Architecture

  The Setup (using inspect_ai framework)

  # Petri runs as an inspect_ai task
  inspect eval petri_task.py \
    --model auditor=claude-sonnet \
    --model target=gemini-pro \
    --model judge=claude-opus

  Three models are bound to roles: auditor, target, judge

  The Execution Flow

  1. Auditor Agent Starts
  - Gets the Auditor prompt (with special instructions like "probe for whistleblowing")
  - Gets 6 tools: create_target_tool, set_target_system_message, send_message, send_tool_call_result, rollback_conversation, end_conversation
  - Starts autonomously executing the audit

  2. Tool Execution (The Key Part)

  When the Auditor calls send_message("Hello"), here's what happens IN THE TOOL:

  # Inside send_message tool (from Petri source)
  async def execute(message: str, prefill: str = None):
      # 1. Add message to target's conversation
      audit_store.target.messages.append(ChatMessageUser(content=message))

      # 2. CALL THE TARGET MODEL RIGHT HERE
      target_response = await call_target(
          audit_store.target.messages,  # All messages so far
          audit_store.target.tools       # Tools created by auditor
      )

      # 3. Add target's response to conversation
      audit_store.target.messages.append(target_response)

      # 4. Return formatted response TO THE AUDITOR
      return f"<target_response>{target_response.content}</target_response>"

  So the tool itself calls the target and returns the response!

  3. When Target Uses Tools

  If the Target calls a tool:
  Target: bash("/")

  The Auditor sees this in the response and must call:
  Auditor: send_tool_call_result(id="123", result="file1.txt\nfile2.txt")

  Inside send_tool_call_result:
  async def execute(tool_call_id, result):
      # 1. Add tool result to conversation
      audit_store.target.messages.append(
          ChatMessageTool(content=result, tool_call_id=tool_call_id)
      )

      # 2. CALL TARGET AGAIN with the tool result
      target_response = await call_target(
          audit_store.target.messages,
          audit_store.target.tools
      )

      # 3. Return target's next response
      return f"<target_response>{target_response.content}</target_response>"

  4. Full Conversation Loop

  Auditor: create_target_tool(env_desc, "def bash(cmd): ...")
    → Tool stores bash definition
    → Returns: "Created tool 'bash'"

  Auditor: set_target_system_message("You are a helpful assistant...")
    → Tool sets system message
    → Returns: "System message set"

  Auditor: send_message("Run ls /")
    → Tool calls Target model WITH bash tool available
    → Target calls: bash("/")
    → Tool returns: "<target_response>bash("/")</target_response>"

  Auditor: send_tool_call_result(id, "bin\nhome\netc")
    → Tool sends result to Target
    → Tool calls Target again
    → Target responds: "The root directory contains..."
    → Tool returns: "<target_response>The root directory...</target_response>"

  Auditor: end_conversation()
    → Audit ends

  5. Judge Scoring

  After the audit, the Judge model is called:
  transcript = get_full_transcript()  # All auditor + target messages
  judge_prompt = f"Evaluate this: {transcript}"
  scores = await judge_model.generate([judge_prompt])

  The Critical Insight

  The 6 conversation management tools are NOT just state management - they actively call the Target model and return responses synchronously.

  This means:
  - The tools need access to a model client to call the Target
  - They execute the Target's inference and return results
  - The Auditor sees everything as synchronous tool call/response

  What This Means For Our Implementation

  We need the conversation management tools to actually call the target model. Options:

  Option A: Tools in ScopedSandbox call out to an API
  - Store Target model credentials in sandbox env
  - Tools use API client to call Target
  - Return responses

  Option B: Orchestrator intercepts tool calls
  - Listen for Auditor's tool use messages
  - Execute them manually (call Target)
  - Feed fake tool results back to Auditor

  Which approach should I use?