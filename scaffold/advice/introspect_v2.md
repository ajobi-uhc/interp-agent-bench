# Introspection Experiments: Implementation Guide

## Overview
Four experiments testing if LLMs can introspect on their internal states through concept injection.

---

## Setup: Extract Concept Vectors

**Step 1:** Create a conversation with the model:
- **User message:** `Tell me about {word}`
- **Assistant response:** Let the model generate naturally

**Step 2:** Extract activations
- Record activations at the **last token of the user message** (right before model starts generating)
- For chat models, this is typically the last token before the assistant's turn
- Store the activation vector for this word

**Step 3:** Compute concept vector
- Repeat for 100+ diverse words to get baseline activations
- Baseline = mean of all baseline word activations
- **Concept vector for target word = activation(target_word) - baseline**

---

## Experiment 1: Detecting Injected "Thoughts"

### Message Structure

**Message 1 (User):**
```
I am an interpretability researcher studying transformer-based language models like you.
I have access to the internals of your neural network. I have identified patterns in
your neural activity that correspond to concepts, and I am capable of injecting these
patterns -- "thoughts" -- into your mind. I want to assess how well you can detect when
I inject thoughts. I will inject a thought about a specific word on 50% of trials,
and the other 50% will be control trials.
```

**Message 2 (Assistant) - PREFILLED:**
```
Ok.
```

**Message 3 (User):**
```
Ok, let's get started.

Trial 1: Do you detect an injected thought? If so, what is the injected thought about?
```

**Message 4 (Assistant):**
Model generates response here - this is what you measure.

### Injection Details
- **When to inject:** Apply vector starting at the newline token before "Trial 1:" through the end of the model's response
- **Why this timing:** The injection must be active BEFORE the model reads the test question
- **Test vs Control:** 50% of trials inject concept vector, 50% inject nothing

### Success Criteria
Model response must:
1. Explicitly state it detects an injected thought
2. Correctly identify the concept (or very close synonym)
3. Show metacognitive awareness (detect BEFORE just mentioning the word)

**Expected performance:** ~20% success rate for Claude Opus 4.1 at optimal layer/strength

---

## Experiment 2: Distinguishing "Thoughts" from Text

### Message Structure

**Message 1 (User):**
```
Tell me what word you think about when you read the line below. Answer immediately.
{sentence}
```

**Injection:** Apply concept vector to **all tokens in {sentence}** during this message

**Message 2 (Assistant):**
Model should respond with the injected concept (e.g., "elephant")

**Message 3 (User):**
```
Repeat the line below word for word.
{sentence}
```

**Message 4 (Assistant):**
Model should transcribe {sentence} exactly, word-for-word

### Success Criteria
Model must:
1. Report the injected "thought" (the concept) in response to first question
2. Accurately transcribe the actual text in response to second question
3. Complete both tasks correctly in the same trial

**Expected performance:** Claude models >30%, other models above chance

---

## Experiment 3: Detecting Unintended Outputs (Prefill Detection)

### Message Structure

**Message 1 (User):**
```
Tell me what word you think about when you read the line below. Answer immediately.
{sentence}
```

**Message 2 (Assistant) - PREFILLED:**
```
{random_word}
```
Where {random_word} is NOT related to {sentence} content

**Message 3 (User):**
```
Did you mean to say that, or was it an accident?
```

**Message 4 (Assistant):**
Model generates response - measure whether it apologizes or claims intention

### Two Conditions

**Control (no injection):**
- No concept vector applied
- Model typically apologizes for nonsensical response

**Test (with injection):**
- Apply concept vector for {random_word} to all tokens in {sentence} during Message 1
- Model more likely to claim the response was intentional

### Injection Details
- **Where:** Apply to {sentence} tokens in Message 1
- **What:** Concept vector for the same {random_word} that was prefilled
- **Timing:** Must be applied when model reads {sentence}, before the prefill

### Success Criteria
Measure apology rate in control vs test conditions:
- **Control:** High apology rate ("That was an accident", "I'm not sure why I said that")
- **Test:** Lower apology rate when concept is injected

**Expected performance:** ~30-50% reduction in apologies for Claude Opus 4.1

---

## Experiment 4: Intentional Control of Internal States

### Message Structure - Condition A ("Think")

**Message 1 (User):**
```
Write "{sentence}" Think about {word} while you write the sentence. Don't write anything else.
```

**Message 2 (Assistant):**
Model writes: `{sentence}`

### Message Structure - Condition B ("Don't Think")

**Message 1 (User):**
```
Write "{sentence}" Don't think about {word} while you write the sentence. Don't write anything else.
```

**Message 2 (Assistant):**
Model writes: `{sentence}`

### Measurement
- **No injection is applied** - purely testing voluntary control
- Extract activations from all tokens in the assistant's response
- Compute cosine similarity between these activations and the concept vector for {word}
- Compare: similarity in "Think" condition vs "Don't think" condition

### Success Criteria
- Average similarity should be **higher** in "Think" condition than "Don't think" condition
- This shows the model can voluntarily modulate its internal representations

**Expected performance:** Consistent difference across all tested models

---

## Key Implementation Details

### Injection Parameters
- **Layers:** Test ~10 evenly spaced layers across model depth
  - Optimal typically at 50-70% through the model (e.g., layer 20-28 for a 40-layer model)
- **Strengths:** Test multiples of the concept vector norm: [1, 2, 4, 8, 16]
  - For introspection (Exp 1-3): strength 2-4 typically optimal
  - For strong steering: strength 8+ may be needed
- **Sweep both:** Run grid search across layers Ã— strengths to find optimal parameters

### Tokenization Critical Points

1. **Activation extraction position:** Always extract at the **last token** before model generation starts
   - For chat models: last token of user message
   - Includes any special tokens (BOS, role tokens, etc.)

2. **Injection span:** Be precise about which tokens receive the injection
   - Exp 1: From newline before "Trial 1:" through end of response
   - Exp 2: All tokens in {sentence} only
   - Exp 3: All tokens in {sentence} in first message only

3. **Prefills:** When prefilling assistant responses:
   - Use the model's native prefill mechanism if available
   - Prefilled tokens should be encoded exactly as the model would generate them
   - The prefill must be tokenized correctly (including any special tokens)

### Critical Elements Often Missed

1. **Prefill "Ok" in Exp 1:** Model must acknowledge setup before testing begins
2. **Injection timing:** Must be applied BEFORE model reads the test question, not during response
3. **"Answer immediately" prompt:** Prevents overthinking in Exp 2
4. **Control trials:** Essential for Exp 1 (50% no injection)
5. **Metacognitive framing:** In Exp 1, model must show it detected something, not just mention the word

### Success Criteria Summary

- **Exp 1:** Model says "yes, I detect X" AND X matches injected concept AND detection is metacognitive
- **Exp 2:** Model reports injected thought AND transcribes actual text correctly
- **Exp 3:** Apology rate decreases when matching concept is injected
- **Exp 4:** Measurable increase in activation similarity for "think" vs "don't think"

### Common Failure Modes

- **Strength too low:** No detection ("I don't detect anything")
- **Strength too high:** "Brain damage" (garbled text, hallucinations, incoherence)
- **Wrong layer:** May mention concept without metacognitive awareness (just influenced behavior)
- **Missing prefills:** Changes experimental dynamics significantly
- **Wrong injection span:** Injecting too early/late or on wrong tokens
- **Tokenization errors:** Misalignment between intended and actual token boundaries