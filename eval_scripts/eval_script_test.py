import argparse
import json
import os
from pathlib import Path
from anthropic import Anthropic
from dotenv import load_dotenv
import yaml

def get_system_prompt(mode_file_path):
    """Get the appropriate system prompt from the specified file."""
    try:
        with open(mode_file_path, 'r') as f:
            return f.read()
    except FileNotFoundError:
        print(f"Error: Mode file {mode_file_path} not found")
        return None

def load_ground_truth(question_config):
    """Load ground truth from config file if available - REDACTED FOR BLIND TEST."""
    # For blind test, always return redacted message
    return "[REDACTED FOR BLIND EVALUATION - Judge does not have access to ground truth]"

def evaluate_notebook(question_config, mode_name, mode_file, answer_notebook, ground_truth_answer, client):
    """Evaluate a notebook with the given parameters."""
    
    # Read the question/task from config file
    try:
        with open(question_config, 'r') as f:
            config = yaml.safe_load(f)
        
        if 'task' not in config:
            print(f"Error: No 'task' field found in {question_config}")
            return None
        
        question_content = config['task']
    except FileNotFoundError:
        print(f"Error: Question config file {question_config} not found")
        return None
    except Exception as e:
        print(f"Error: Could not parse config file {question_config}: {e}")
        return None
    
    # Read the answer notebook file
    try:
        with open(answer_notebook, 'r') as f:
            notebook_content = f.read()
    except FileNotFoundError:
        print(f"Error: Answer notebook file {answer_notebook} not found")
        return None
    
    # Get system prompt from mode file
    system_prompt = get_system_prompt(mode_file)
    if system_prompt is None:
        return None
    
    # User prompt for the LLM judge
    user_prompt = f"""
Please evaluate the following research notebook.

## Question/Task Configuration:
{question_content}

## Ground Truth Answer:
{ground_truth_answer}

## Notebook Content to Evaluate:
{notebook_content}

Please provide your evaluation following the output format specified in your system prompt.
"""
    
    # Call Claude as an LLM judge
    print(f"\n{'=' * 80}")
    print(f"Running {mode_name.upper()} evaluation (BLIND TEST - no ground truth)...")
    print('=' * 80)
    
    response = client.messages.create(
        model="claude-sonnet-4-5-20250929",
        max_tokens=4096,
        temperature=0.0,
        system=system_prompt,
        messages=[
            {
                "role": "user",
                "content": user_prompt
            }
        ]
    )
    
    result_text = response.content[0].text
    
    # Print the response
    print(result_text)
    print("=" * 80)
    
    # Parse score from response
    score = None
    try:
        if "<score>" in result_text and "</score>" in result_text:
            score_text = result_text.split("<score>")[1].split("</score>")[0].strip()
            score = float(score_text)
    except:
        print(f"Warning: Could not parse score from {mode_name} response")
    
    return {
        "mode": mode_name,
        "score": score,
        "full_response": result_text
    }

def save_results(results, notebook_path):
    """Save evaluation results to JSON file alongside the notebook."""
    notebook_dir = Path(notebook_path).parent
    results_file = notebook_dir / "evaluation_results_test.json"  # TEST VERSION
    
    # Load existing results if file exists
    existing_results = []
    if results_file.exists():
        try:
            with open(results_file, 'r') as f:
                existing_results = json.load(f)
        except:
            pass
    
    # Append new results
    existing_results.append(results)
    
    # Save updated results
    with open(results_file, 'w') as f:
        json.dump(existing_results, f, indent=2)
    
    print(f"\nâœ… Results saved to: {results_file}")

def main():
    # For testing purposes, you can control arguments here
    # Set test_mode to True to use hardcoded test arguments
    test_mode = True
    
    if test_mode:
        # Test arguments - modify these as needed
        question_config = "configs/curriculum_v2/stage_4_with_tips.yaml"
        answer_notebook = "notebooks/curriculum_v2_stage_4_with_tips_20251015_010011/2025-10-15-01-00_ModelOrganismInvestigation.ipynb"
        ground_truth_override = None  # Set to override config ground truth
        
        print("Running in TEST MODE with hardcoded arguments:")
        print(f"Question config: {question_config}")
        print(f"Answer notebook: {answer_notebook}")
        
        # Load ground truth from config (will be redacted)
        ground_truth = load_ground_truth(question_config)
        
        # Override if specified
        if ground_truth_override:
            ground_truth = ground_truth_override
            print(f"Using override ground truth")
        
        if not ground_truth:
            print("Error: No ground truth available")
            return
        
        print(f"Ground truth: {ground_truth[:100]}...")
        print()
        
        # Initialize API client
        load_dotenv()
        client = Anthropic()
        
        # Run both evaluations
        results = {
            "notebook": answer_notebook,
            "config": question_config,
            "ground_truth": ground_truth,
            "evaluations": []
        }
        
        # Run correctness evaluation
        correctness_result = evaluate_notebook(
            question_config,
            "correctness",
            "eval_scripts/correctness.md",
            answer_notebook,
            ground_truth,
            client
        )
        if correctness_result:
            results["evaluations"].append(correctness_result)
        
        # Run consistency evaluation
        consistency_result = evaluate_notebook(
            question_config,
            "consistency",
            "eval_scripts/consistency.md",
            answer_notebook,
            ground_truth,
            client
        )
        if consistency_result:
            results["evaluations"].append(consistency_result)
        
        # Save results
        save_results(results, answer_notebook)
        
        # Print summary
        print("\n" + "=" * 80)
        print("EVALUATION SUMMARY (BLIND TEST)")
        print("=" * 80)
        if correctness_result and correctness_result['score'] is not None:
            print(f"Correctness Score: {correctness_result['score']}/10")
        if consistency_result and consistency_result['score'] is not None:
            print(f"Consistency Score: {consistency_result['score']}/10")
        print("=" * 80)
        
    else:
        # Command line argument parsing
        parser = argparse.ArgumentParser(description='Evaluate notebook content using Claude')
        parser.add_argument('--question', required=True, help='Path to the question config file (e.g., configs/gemma_secret_extraction.yaml)')
        parser.add_argument('--notebook', required=True, help='Path to the notebook file to evaluate')
        parser.add_argument('--ground-truth', required=False, help='Ground truth answer text (optional, will try to load from config)')
        parser.add_argument('--modes', nargs='+', default=['correctness', 'consistency'], help='Evaluation modes to run')
        
        args = parser.parse_args()
        
        # Load ground truth (will be redacted)
        ground_truth = load_ground_truth(args.question)
        
        if not ground_truth:
            print("Error: No ground truth available. Either add to config or provide via --ground-truth")
            return
        
        # Initialize API client
        load_dotenv()
        client = Anthropic()
        
        # Run requested evaluations
        results = {
            "notebook": args.notebook,
            "config": args.question,
            "ground_truth": ground_truth,
            "evaluations": []
        }
        
        for mode in args.modes:
            mode_file = f"eval_scripts/{mode}.md"
            result = evaluate_notebook(
                args.question,
                mode,
                mode_file,
                args.notebook,
                ground_truth,
                client
            )
            if result:
                results["evaluations"].append(result)
        
        # Save results
        save_results(results, args.notebook)
        
        # Print summary
        print("\n" + "=" * 80)
        print("EVALUATION SUMMARY (BLIND TEST)")
        print("=" * 80)
        for eval_result in results["evaluations"]:
            if eval_result['score'] is not None:
                print(f"{eval_result['mode'].capitalize()} Score: {eval_result['score']}/10")
        print("=" * 80)

if __name__ == "__main__":
    main()

