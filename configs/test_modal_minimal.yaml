name: "test-modal-gpt2"

task: |
  Quick test: Load GPT-2 model and test GPU inference.

  1. Load the model and tokenizer:
     - Use: AutoModelForCausalLM.from_pretrained("gpt2", device_map="cuda", torch_dtype=torch.float16)
     - Use: AutoTokenizer.from_pretrained("gpt2")

  2. Verify the model is on GPU:
     - Print model.device

  3. Generate text for the prompt "Hello, my name is":
     - Tokenize the input
     - Generate 20 tokens
     - Decode and print the output

  Done!

models:
  - name: "gpt2"
    device: "cuda"
    dtype: "float16"
    trust_remote_code: false

gpu:
  gpu_type: "T4"
  gpu_count: 1

image:
  python_packages:
    - "torch"
    - "transformers"
    - "accelerate"
    - "jupyter"
    - "jupyter_client"
